{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN20(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(5, 3), stride=(3, 1), padding=(4, 1), dilation=(2, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=True)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=46080, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import get_years_dataset, train_val_split\n",
    "import yaml\n",
    "from model import CNN20\n",
    "params = yaml.safe_load(open('./params.yaml', 'r')) \n",
    "seed = params['global']['seed']\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "device = params['global']['device']\n",
    "year_start = params['dataset']['year_start']\n",
    "year_split = params['dataset']['year_split'] \n",
    "year_end = params['dataset']['year_end']\n",
    "ret_days = params['dataset']['ret_days']\n",
    "ratio = params['dataset']['train_val_split_ratio']\n",
    "num_workers = params['train']['num_workers'] \n",
    "batch_size = params['train']['batch_size']\n",
    "model_name = 'baseline_I20R5'\n",
    "model = CNN20()\n",
    "model.load_state_dict(torch.load(f\"./{model_name}_best_model.pth\", weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Length of 1993-1999 datasets: 693037\n",
      "693037\n",
      "[INFO]Length of 2000-2019 datasets: 1497687\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = train_val_split(get_years_dataset('./monthly_20d', year_start, year_split+1, ret_days), ratio, generator)\n",
    "test_dataset = get_years_dataset('./monthly_20d', year_split+1, year_end+1, ret_days)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11701it [00:38, 301.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   \n",
    "model.to(device)\n",
    "TP = TN = FP = FN = 0\n",
    "model.eval()\n",
    "for i, (images, labels) in tqdm(enumerate(test_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            TP += ((pred == labels) & (labels == 1)).sum()\n",
    "            TN += ((pred == labels) & (labels == 0)).sum()\n",
    "            FP += ((pred != labels) & (labels == 0)).sum()\n",
    "            FN += ((pred != labels) & (labels == 1)).sum()\n",
    "TOT = TP + TN + FP + FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5302\n",
      "TP: 355259, TN: 438811, FP: 296658, FN: 406959\n",
      "Precision: 0.5449\n",
      "Recall: 0.4661\n",
      "F1 Score: 0.5024\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Accuracy = (TP + TN) / TOT\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date StockID   MarketCap        Ret_5d   Ret_20d   Ret_60d  \\\n",
      "0     2017-01-31   10001    133078.0  4.370390e-07 -0.000002 -0.005954   \n",
      "1     2017-02-28   10001    133078.0  3.951997e-03  0.002795  0.009953   \n",
      "2     2017-03-31   10001    133604.0 -7.874612e-03 -0.015749  0.021723   \n",
      "3     2017-04-28   10001    131500.0  9.999880e-03  0.016001  0.038072   \n",
      "4     2017-05-31   10001    133604.0  4.370390e-07  0.021722       NaN   \n",
      "...          ...     ...         ...           ...       ...       ...   \n",
      "67853 2017-08-31   93436  59395080.0 -3.512252e-02 -0.041584 -0.109835   \n",
      "67854 2017-09-29   93436  57310600.0  4.626140e-02 -0.059308 -0.069807   \n",
      "67855 2017-10-31   93436  55719252.0 -7.685617e-02 -0.072363  0.054292   \n",
      "67856 2017-11-30   93436  51907496.0  7.737713e-03  0.008093  0.110766   \n",
      "67857 2017-12-29   93436  52554948.0  8.048780e-02  0.110713 -0.172055   \n",
      "\n",
      "       Ret_month  EWMA_vol  \n",
      "0      -0.000002  0.000450  \n",
      "1       0.009953  0.000180  \n",
      "2      -0.015749  0.000064  \n",
      "3       0.016001  0.000030  \n",
      "4       0.023703  0.000015  \n",
      "...          ...       ...  \n",
      "67853  -0.041584  0.000483  \n",
      "67854  -0.028057  0.000462  \n",
      "67855  -0.068411  0.000365  \n",
      "67856   0.008093  0.000455  \n",
      "67857   0.137981  0.000389  \n",
      "\n",
      "[67858 rows x 8 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "year = 2017\n",
    "dir = './monthly_20d'\n",
    "labels_path = os.path.join(f'{dir}/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather')\n",
    "labels_df = pd.read_feather(labels_path)\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96} \n",
    "\n",
    "images_path = os.path.join(f'{dir}/20d_month_has_vb_[20]_ma_{2017}_images.dat')\n",
    "images = np.memmap(images_path, dtype=np.uint8, mode='r')\n",
    "images = images.reshape((-1,1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "plt.imshow(np.squeeze(images[1]), cmap='gray')\n",
    "\n",
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "dir = './monthly_20d'\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}    \n",
    "model.eval()\n",
    "count = 0 \n",
    "output = []\n",
    "with torch.no_grad():\n",
    "    for year in range(2000, 2020):\n",
    "        images_path = os.path.join(f'{dir}/20d_month_has_vb_[20]_ma_{year}_images.dat')\n",
    "        images = np.memmap(images_path, dtype=np.uint8, mode='r')\n",
    "        images = images.reshape((-1,1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "        count += images.shape[0]\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i].copy()\n",
    "        image = torch.from_numpy(image).to(device)\n",
    "        image = image.float()\n",
    "        output.append(model(image.unsqueeze(0)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2000: Found 394 missing labels.\n",
      "Year 2000: Added 97754 records with non-missing return values.\n",
      "Year 2001: Found 346 missing labels.\n",
      "Year 2001: Added 91639 records with non-missing return values.\n",
      "Year 2002: Found 261 missing labels.\n",
      "Year 2002: Added 85468 records with non-missing return values.\n",
      "Year 2003: Found 239 missing labels.\n",
      "Year 2003: Added 80655 records with non-missing return values.\n",
      "Year 2004: Found 208 missing labels.\n",
      "Year 2004: Added 79304 records with non-missing return values.\n",
      "Year 2005: Found 231 missing labels.\n",
      "Year 2005: Added 79652 records with non-missing return values.\n",
      "Year 2006: Found 233 missing labels.\n",
      "Year 2006: Added 79930 records with non-missing return values.\n",
      "Year 2007: Found 326 missing labels.\n",
      "Year 2007: Added 80814 records with non-missing return values.\n",
      "Year 2008: Found 378 missing labels.\n",
      "Year 2008: Added 77808 records with non-missing return values.\n",
      "Year 2009: Found 180 missing labels.\n",
      "Year 2009: Added 69885 records with non-missing return values.\n",
      "Year 2010: Found 185 missing labels.\n",
      "Year 2010: Added 67926 records with non-missing return values.\n",
      "Year 2011: Found 170 missing labels.\n",
      "Year 2011: Added 67155 records with non-missing return values.\n",
      "Year 2012: Found 173 missing labels.\n",
      "Year 2012: Added 66077 records with non-missing return values.\n",
      "Year 2013: Found 119 missing labels.\n",
      "Year 2013: Added 65367 records with non-missing return values.\n",
      "Year 2014: Found 131 missing labels.\n",
      "Year 2014: Added 67171 records with non-missing return values.\n",
      "Year 2015: Found 167 missing labels.\n",
      "Year 2015: Added 68660 records with non-missing return values.\n",
      "Year 2016: Found 214 missing labels.\n",
      "Year 2016: Added 67927 records with non-missing return values.\n",
      "Year 2017: Found 182 missing labels.\n",
      "Year 2017: Added 67676 records with non-missing return values.\n",
      "Year 2018: Found 163 missing labels.\n",
      "Year 2018: Added 68318 records with non-missing return values.\n",
      "Year 2019: Found 136 missing labels.\n",
      "Year 2019: Added 68501 records with non-missing return values.\n",
      "Records per year:\n",
      "year\n",
      "2000    97754\n",
      "2001    91639\n",
      "2002    85468\n",
      "2003    80655\n",
      "2004    79304\n",
      "2005    79652\n",
      "2006    79930\n",
      "2007    80814\n",
      "2008    77808\n",
      "2009    69885\n",
      "2010    67926\n",
      "2011    67155\n",
      "2012    66077\n",
      "2013    65367\n",
      "2014    67171\n",
      "2015    68660\n",
      "2016    67927\n",
      "2017    67676\n",
      "2018    68318\n",
      "2019    68501\n",
      "Name: count, dtype: int64\n",
      "Successfully created combined DataFrame with 1497687 total records\n",
      "              Date StockID    MarketCap        Ret_5d   Ret_20d   Ret_60d  \\\n",
      "0       2000-01-31   10001     19906.25 -3.374006e-07  0.015383  0.031105   \n",
      "1       2000-02-29   10001     20212.50 -7.576549e-03 -0.030675 -0.007593   \n",
      "2       2000-03-31   10001    -19712.00  1.562460e-02  0.031251  0.007632   \n",
      "3       2000-04-28   10001    -19943.00  1.930500e-02 -0.011582  0.003799   \n",
      "4       2000-05-31   10001    -19481.00 -1.976308e-02 -0.036619  0.027606   \n",
      "...            ...     ...          ...           ...       ...       ...   \n",
      "1497682 2019-08-30   93436  40412844.00  2.739224e-02  0.067639  0.490805   \n",
      "1497683 2019-09-30   93436  43356600.00 -1.307759e-02  0.360527  0.765477   \n",
      "1497684 2019-10-31   93436  56762756.00  6.547814e-02  0.047695  0.844879   \n",
      "1497685 2019-11-29   93436  59470036.00  1.803443e-02  0.256895  1.057951   \n",
      "1497686 2019-12-31   93436  75717728.00  1.764403e-01  0.531828  0.229557   \n",
      "\n",
      "         Ret_month  EWMA_vol  year  \n",
      "0         0.015383  0.000659  2000  \n",
      "1        -0.015289  0.000329  2000  \n",
      "2         0.011720  0.000577  2000  \n",
      "3        -0.023165  0.000553  2000  \n",
      "4         0.027606  0.000344  2000  \n",
      "...            ...       ...   ...  \n",
      "1497682   0.067639  0.000867  2019  \n",
      "1497683   0.307429  0.000920  2019  \n",
      "1497684   0.047695  0.002130  2019  \n",
      "1497685   0.267897  0.001075  2019  \n",
      "1497686        NaN  0.000757  2019  \n",
      "\n",
      "[1497687 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "dir = './monthly_20d'\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}    \n",
    "model.eval()\n",
    "count = 0 \n",
    "dfs = []\n",
    "with torch.no_grad():\n",
    "    for year in range(2000, 2020):\n",
    "        labels_path = os.path.join(f'{dir}/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather')\n",
    "        images_path = os.path.join(f'{dir}/20d_month_has_vb_[20]_ma_{year}_images.dat')\n",
    "        images = np.memmap(images_path, dtype=np.uint8, mode='r')\n",
    "        images = images.reshape((-1,1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "        labels_df = pd.read_feather(labels_path)\n",
    "        labels = labels_df[f'Ret_{ret_days}d']\n",
    "        missing = labels.isna()\n",
    "        print(f\"Year {year}: Found {missing.sum()} missing labels.\")\n",
    "        filtered_df = labels_df[~missing].copy()\n",
    "        if 'year' not in filtered_df.columns:\n",
    "            filtered_df['year'] = year\n",
    "        dfs.append(filtered_df)\n",
    "        print(f\"Year {year}: Added {len(filtered_df)} records with non-missing return values.\")\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Successfully created combined DataFrame with {len(df)} total records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11701it [00:47, 248.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51771796 0.48228204]\n",
      " [0.56659216 0.4334078 ]\n",
      " [0.4586978  0.5413022 ]\n",
      " ...\n",
      " [0.5462434  0.45375657]\n",
      " [0.5095201  0.4904799 ]\n",
      " [0.40733334 0.5926667 ]]\n",
      "(1497687, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad(): \n",
    "    for i, (images, labels) in tqdm(enumerate(test_loader)):\n",
    "        images = images.to(device)\n",
    "        predictions = F.softmax(model(images), dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy()) \n",
    "\n",
    "final_predictions = np.concatenate(all_predictions, axis=0)\n",
    "print(final_predictions)\n",
    "print(final_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Equal-Weight_D1  Value-Weight_D1  Equal-Weight_D2  \\\n",
      "0   2000-01-31         0.012843        -0.005811         0.031171   \n",
      "1   2000-02-29         0.001189        -0.003515         0.031863   \n",
      "2   2000-03-31        -0.041435        -0.063153        -0.029527   \n",
      "3   2000-04-28        -0.003569         0.008126        -0.002547   \n",
      "4   2000-05-31         0.016829         0.041189         0.042706   \n",
      "..         ...              ...              ...              ...   \n",
      "235 2019-08-30         0.009135         0.025464         0.020704   \n",
      "236 2019-09-30        -0.024527        -0.010425        -0.017461   \n",
      "237 2019-10-31         0.001681        -0.005526         0.008579   \n",
      "238 2019-11-29        -0.009434         0.006945         0.001252   \n",
      "239 2019-12-31         0.004929         0.000222         0.006255   \n",
      "\n",
      "     Value-Weight_D2  Equal-Weight_D3  Value-Weight_D3  Equal-Weight_D4  \\\n",
      "0           0.028903         0.046331         0.030648         0.053292   \n",
      "1           0.010540         0.036343         0.057136         0.035115   \n",
      "2          -0.016814        -0.023513        -0.009600        -0.025971   \n",
      "3          -0.002584        -0.001251        -0.020948         0.000882   \n",
      "4           0.042976         0.048630         0.007845         0.070076   \n",
      "..               ...              ...              ...              ...   \n",
      "235         0.011995         0.020705         0.019581         0.022092   \n",
      "236        -0.016382        -0.019674        -0.006054        -0.017973   \n",
      "237         0.000880         0.011040         0.020342         0.017717   \n",
      "238        -0.002690         0.002635        -0.004503         0.006177   \n",
      "239         0.005385         0.007715         0.011316         0.009731   \n",
      "\n",
      "     Value-Weight_D4  Equal-Weight_D5  ...  Equal-Weight_D7  Value-Weight_D7  \\\n",
      "0           0.035177         0.044893  ...         0.055227         0.057494   \n",
      "1           0.011608         0.048365  ...         0.038756         0.023122   \n",
      "2          -0.010684        -0.023867  ...        -0.014562        -0.023113   \n",
      "3          -0.010497         0.000687  ...         0.016270        -0.010695   \n",
      "4           0.038642         0.076551  ...         0.092933         0.061867   \n",
      "..               ...              ...  ...              ...              ...   \n",
      "235         0.004591         0.022373  ...         0.026939         0.023444   \n",
      "236        -0.010999        -0.015136  ...        -0.019861        -0.015142   \n",
      "237         0.007415         0.016084  ...         0.022595         0.020342   \n",
      "238         0.002613         0.008386  ...         0.003304        -0.000672   \n",
      "239         0.006673         0.003066  ...         0.008359         0.006363   \n",
      "\n",
      "     Equal-Weight_D8  Value-Weight_D8  Equal-Weight_D9  Value-Weight_D9  \\\n",
      "0           0.057988         0.038646         0.052400         0.060870   \n",
      "1           0.039520         0.023374         0.040430        -0.002602   \n",
      "2          -0.002250         0.018493         0.006345         0.010297   \n",
      "3           0.015279        -0.009021         0.014177        -0.000538   \n",
      "4           0.088558         0.062606         0.092790         0.062748   \n",
      "..               ...              ...              ...              ...   \n",
      "235         0.024360         0.021035         0.026153         0.015092   \n",
      "236        -0.017745        -0.007282        -0.016228        -0.014495   \n",
      "237         0.018763         0.023915         0.026388         0.032852   \n",
      "238         0.002445        -0.000482         0.007497         0.007072   \n",
      "239         0.008032         0.004311         0.009170         0.013964   \n",
      "\n",
      "     Equal-Weight_D10  Value-Weight_D10  Equal-Weight_H-L  Value-Weight_H-L  \n",
      "0            0.062438          0.044436          0.049595          0.050247  \n",
      "1            0.019014         -0.044533          0.017825         -0.041018  \n",
      "2            0.021709          0.031426          0.063144          0.094578  \n",
      "3            0.021438         -0.010650          0.025007         -0.018775  \n",
      "4            0.090680          0.073228          0.073851          0.032039  \n",
      "..                ...               ...               ...               ...  \n",
      "235          0.027155          0.014021          0.018020         -0.011442  \n",
      "236         -0.012801         -0.019093          0.011726         -0.008668  \n",
      "237          0.027200          0.021809          0.025519          0.027335  \n",
      "238          0.007111          0.001267          0.016545         -0.005677  \n",
      "239          0.016990          0.018635          0.012061          0.018413  \n",
      "\n",
      "[240 rows x 23 columns]\n",
      "Average Returns: {'Equal-Weight_D1': np.float64(-0.007831495516329568), 'Value-Weight_D1': np.float64(-0.003554743805149371), 'Equal-Weight_D2': np.float64(-0.0014967916758015676), 'Value-Weight_D2': np.float64(-0.0007772914268318547), 'Equal-Weight_D3': np.float64(0.00019093092690734847), 'Value-Weight_D3': np.float64(-9.298081604109942e-05), 'Equal-Weight_D4': np.float64(0.001853284165746195), 'Value-Weight_D4': np.float64(0.00020589543933777858), 'Equal-Weight_D5': np.float64(0.0028406350447470895), 'Value-Weight_D5': np.float64(0.0008711057488303868), 'Equal-Weight_D6': np.float64(0.0036155367820924554), 'Value-Weight_D6': np.float64(0.0012559515621809536), 'Equal-Weight_D7': np.float64(0.004075227071568586), 'Value-Weight_D7': np.float64(0.0012182957462570213), 'Equal-Weight_D8': np.float64(0.004886894046799165), 'Value-Weight_D8': np.float64(0.0020391016931310614), 'Equal-Weight_D9': np.float64(0.0070283290523246045), 'Value-Weight_D9': np.float64(0.0031106857085806978), 'Equal-Weight_D10': np.float64(0.009929403973638366), 'Value-Weight_D10': np.float64(0.003966607402725045), 'Equal-Weight_H-L': np.float64(0.017760899489967934), 'Value-Weight_H-L': np.float64(0.007521351207874417)}\n",
      "Average Annualized Returns: {'Equal-Weight_D1': np.float64(-0.39470737402301026), 'Value-Weight_D1': np.float64(-0.1791590877795283), 'Equal-Weight_D2': np.float64(-0.07543830046039901), 'Value-Weight_D2': np.float64(-0.03917548791232548), 'Equal-Weight_D3': np.float64(0.009622918716130363), 'Value-Weight_D3': np.float64(-0.00468623312847141), 'Equal-Weight_D4': np.float64(0.09340552195360823), 'Value-Weight_D4': np.float64(0.01037713014262404), 'Equal-Weight_D5': np.float64(0.1431680062552533), 'Value-Weight_D5': np.float64(0.04390372974105149), 'Equal-Weight_D6': np.float64(0.18222305381745973), 'Value-Weight_D6': np.float64(0.06329995873392007), 'Equal-Weight_D7': np.float64(0.20539144440705673), 'Value-Weight_D7': np.float64(0.061402105611353876), 'Equal-Weight_D8': np.float64(0.24629945995867794), 'Value-Weight_D8': np.float64(0.10277072533380549), 'Equal-Weight_D9': np.float64(0.35422778423716006), 'Value-Weight_D9': np.float64(0.15677855971246715), 'Equal-Weight_D10': np.float64(0.5004419602713737), 'Value-Weight_D10': np.float64(0.19991701309734225), 'Equal-Weight_H-L': np.float64(0.8951493342943838), 'Value-Weight_H-L': np.float64(0.3790761008768706)}\n"
     ]
    }
   ],
   "source": [
    "df_with_pred = df.copy()\n",
    "df_with_pred[f'pred_{ret_days}d'] = final_predictions[:, 1]\n",
    "df_with_pred['StockID'] = df_with_pred['StockID'].astype('category')\n",
    "portfolio_returns = []\n",
    "annualization_factor = 252 / ret_days\n",
    "for date, date_group in df_with_pred.groupby('Date'):\n",
    "    #print(f\"Date: {date}\")\n",
    "    #print(date_group[['StockID', f'Ret_{ret_days}d', f'pred_{ret_days}d']].head(10))\n",
    "    date_group['decile'] = pd.qcut(date_group[f'pred_{ret_days}d'], 10, labels=False) + 1\n",
    "    value_weighted_returns = {}\n",
    "    for decile, decile_group in date_group.groupby('decile'):\n",
    "            # Use absolute values for market cap to handle negative values\n",
    "            total_market_cap = decile_group['MarketCap'].abs().sum()\n",
    "            decile_group['weight'] = decile_group['MarketCap'].abs() / total_market_cap\n",
    "            value_weighted_returns[decile] = (decile_group[f\"Ret_{ret_days}d\"] * decile_group['weight']).sum()\n",
    "    value_decile_returns = pd.Series(value_weighted_returns)\n",
    "    decile_returns = date_group.groupby('decile')[f'Ret_{ret_days}d'].mean()\n",
    "    hl_return = decile_returns.iloc[9] - decile_returns.iloc[0]\n",
    "    value_hl_return = value_decile_returns.get(10, 0) - value_decile_returns.get(1, 0)\n",
    "\n",
    "    result = {'Date': date}\n",
    "    for i in range(1, 11):\n",
    "        result[f'Equal-Weight_D{i}'] = decile_returns.get(i, np.nan)\n",
    "        result[f'Value-Weight_D{i}'] = value_decile_returns.get(i, np.nan)\n",
    "    result['Equal-Weight_H-L'] = hl_return\n",
    "    result['Value-Weight_H-L'] = value_hl_return\n",
    "    portfolio_returns.append(result)\n",
    "portfolio_returns = pd.DataFrame(portfolio_returns)  \n",
    "print(portfolio_returns)\n",
    "\n",
    "avg_returns = {col: portfolio_returns[col].mean() for col in portfolio_returns.columns if col != 'Date'} # returns per periodic\n",
    "avg_annualized_returns = {col: ret * annualization_factor for col, ret in avg_returns.items()} # annualized returns\n",
    "print(\"Average Returns:\", avg_returns)\n",
    "print(\"Average Annualized Returns:\", avg_annualized_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Length of 1993-2000 datasets: 786765\n",
      "[INFO]Number of training parameters: 662785\n",
      "[INFO]Training on cuda!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "4301it [00:38, 106.28it/s]/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([79])) that is different to the input size (torch.Size([79, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "4303it [00:38, 112.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training_Loss: 1.1409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1827it [00:06, 294.26it/s]/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([126])) that is different to the input size (torch.Size([126, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "1844it [00:06, 278.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Validation Loss: 0.0618\n",
      "Epoch [1/50], Checkpoint saved at ckpt/regression_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:40, 107.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Training_Loss: 0.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:06, 274.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Validation Loss: 0.0455\n",
      "Epoch [2/50], Checkpoint saved at ckpt/regression_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:41, 104.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Training_Loss: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:07, 257.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Validation Loss: 0.0403\n",
      "Epoch [3/50], Checkpoint saved at ckpt/regression_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:41, 103.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Training_Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:06, 288.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Validation Loss: 0.0411\n",
      "Early_Stopping Counter: 1 / 2\n",
      "Epoch [4/50], Checkpoint saved at ckpt/regression_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:40, 106.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Training_Loss: 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:06, 291.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Validation Loss: 0.0389\n",
      "Epoch [5/50], Checkpoint saved at ckpt/regression_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:38, 111.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Training_Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:06, 288.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Validation Loss: 0.0389\n",
      "Early_Stopping Counter: 1 / 2\n",
      "Epoch [6/50], Checkpoint saved at ckpt/regression_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:40, 106.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Training_Loss: 0.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:06, 286.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Validation Loss: 0.0387\n",
      "Epoch [7/50], Checkpoint saved at ckpt/regression_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:40, 106.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Training_Loss: 0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:07, 255.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Validation Loss: 0.0387\n",
      "Early_Stopping Counter: 1 / 2\n",
      "Epoch [8/50], Checkpoint saved at ckpt/regression_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:39, 110.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Training_Loss: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [00:06, 275.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Validation Loss: 0.0388\n",
      "Early_Stopping Counter: 2 / 2\n",
      "[INFO]Early_Stopping is triggered!\n",
      "[INFO]Early stopping at epoch 9\n",
      "[INFO]Best epoch: 7\n",
      "[INFO]Best validation loss: 0.0387\n",
      "[INFO]Training complete in 6m 60s\n",
      "[INFO]Training losses saved to output/regression_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/root/autodl-tmp/MATH5470_Final_proj/train_utils.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(ckpt_path, f'{name}_{record.best_epoch + 1}.pth')))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from model import CNN20\n",
    "class CNN20(nn.Module):\n",
    "    def __init__(self,\n",
    "                drop_out=0.5,\n",
    "                layers=3,\n",
    "                batch_normalization=True,\n",
    "                xavier_initialization=True,\n",
    "                conv_filters=64,\n",
    "                kernel_sizes=[5, 3],\n",
    "                strides=[3, 1],\n",
    "                dilation=[2, 1],\n",
    "                activation=\"LReLu\"):\n",
    "        super(CNN20, self).__init__()\n",
    "        \n",
    "        # Convert lists to tuples for kernel sizes, strides, and dilation\n",
    "        self.kernel_sizes = tuple(kernel_sizes) if isinstance(kernel_sizes, list) else kernel_sizes\n",
    "        self.strides = tuple(strides) if isinstance(strides, list) else strides\n",
    "        self.dilation = tuple(dilation) if isinstance(dilation, list) else dilation\n",
    "        \n",
    "        # Store other parameters\n",
    "        self.drop_out = drop_out\n",
    "        self.activation_type = activation.lower()\n",
    "        self.conv_filters = conv_filters\n",
    "        self.layers = layers\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.xavier_initialization = xavier_initialization\n",
    "        \n",
    "        # Calculate number of filters for each layer\n",
    "        self.filters = [1] + [conv_filters * (2 ** i) for i in range(layers)]\n",
    "        \n",
    "        # Create activation function\n",
    "        self.activation = self._get_activation()\n",
    "        \n",
    "        # Create convolutional blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        dummy_input = torch.randn(1, 1, 64, 60)\n",
    "        for i in range(layers):\n",
    "            if i == 0:\n",
    "                padding_h = self._calculate_padding(\n",
    "                    kernel_size=self.kernel_sizes[0], \n",
    "                    stride=self.strides[0],           \n",
    "                    dilation=self.dilation[0]         \n",
    "                )\n",
    "                padding_w = self._calculate_padding(\n",
    "                    kernel_size=self.kernel_sizes[1],  \n",
    "                    stride=self.strides[1],          \n",
    "                    dilation=self.dilation[1]        \n",
    "                )\n",
    "            else:\n",
    "                padding_h = (self.kernel_sizes[0] - 1) // 2\n",
    "                padding_w = (self.kernel_sizes[1] - 1) // 2\n",
    "\n",
    "            in_channels = self.filters[i]\n",
    "            out_channels = self.filters[i+1]\n",
    "\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=self.kernel_sizes,\n",
    "                    stride=self.strides if i == 0 else (1, 1),\n",
    "                    dilation=self.dilation if i == 0 else (1, 1),\n",
    "                    padding=(padding_h, padding_w)\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels) if self.batch_normalization else nn.Identity(),\n",
    "                self.activation,\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=(2, 1),\n",
    "                    ceil_mode=True\n",
    "                )\n",
    "            )\n",
    "            dummy_input = block(dummy_input)\n",
    "            self.blocks.append(block)\n",
    "        fc_input_dim = dummy_input.size(1) * dummy_input.size(2) * dummy_input.size(3)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(self.drop_out),\n",
    "            nn.Linear(fc_input_dim, 1)\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _calculate_output_dim(self, input_dim, kernel_size, stride, dilation):\n",
    "        return (input_dim - kernel_size + 2 * self._calculate_padding(kernel_size, stride, dilation)) // stride\n",
    "    \n",
    "    def _calculate_padding(self, kernel_size, stride, dilation):\n",
    "        return ((stride - 1) + dilation * (kernel_size - 1)) // 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_uniform_(m.weight) if self.xavier_initialization else init.zeros_(m.weight)\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight) if self.xavier_initialization else init.zeros_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "    \n",
    "    def _get_activation(self):\n",
    "        activation_map ={\n",
    "            \"lrelu\": nn.LeakyReLU(0.01),\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"elu\": nn.ELU(),\n",
    "            \"selu\": nn.SELU(),\n",
    "            \"celu\": nn.CELU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "        }\n",
    "        if self.activation_type not in activation_map:\n",
    "            raise ValueError(f\"Activation type {self.activation_type} not supported\")\n",
    "        return activation_map[self.activation_type]\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from dataset import get_years_dataset, train_val_split\n",
    "from train_utils import trainer\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "config = yaml.safe_load(open('config.yaml', 'r'))\n",
    "model = CNN20(**config)\n",
    "train_dataset, val_dataset = train_val_split(get_years_dataset('./monthly_20d/', 1993, 2001, 20, regression=True), 0.7, False)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")   \n",
    "\n",
    "total_training_parameters = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        total_training_parameters += param.numel()\n",
    "print(f\"[INFO]Number of training parameters: {total_training_parameters}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "model.to('cuda')\n",
    "model, metrics_df = trainer(model, train_loader, val_loader, criterion, optimizer, 50, 'cuda',\n",
    "                                                    2, 0, 'ckpt', 'regression')\n",
    "torch.save(model.state_dict(), os.path.join('model', f'{'regression'}.pth'))\n",
    "metrics_df.to_csv(f'{'output'}/{'regression'}_metrics.csv', index=False)\n",
    "print(f\"[INFO]Training losses saved to {'output'}/{'regression'}_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>StockID</th>\n",
       "      <th>MarketCap</th>\n",
       "      <th>Ret_5d</th>\n",
       "      <th>Ret_20d</th>\n",
       "      <th>Ret_60d</th>\n",
       "      <th>Ret_month</th>\n",
       "      <th>EWMA_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>10026</td>\n",
       "      <td>2899156.25</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.000988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>10026</td>\n",
       "      <td>2916624.25</td>\n",
       "      <td>-0.021123</td>\n",
       "      <td>0.029734</td>\n",
       "      <td>0.050473</td>\n",
       "      <td>0.026309</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>10026</td>\n",
       "      <td>2988574.50</td>\n",
       "      <td>-0.024616</td>\n",
       "      <td>-0.021972</td>\n",
       "      <td>0.025677</td>\n",
       "      <td>-0.010451</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>10026</td>\n",
       "      <td>2957655.75</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.018959</td>\n",
       "      <td>0.031149</td>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>10026</td>\n",
       "      <td>3026714.75</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.192524</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>10026</td>\n",
       "      <td>3030688.50</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.013980</td>\n",
       "      <td>0.193034</td>\n",
       "      <td>0.154645</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>10026</td>\n",
       "      <td>3501411.50</td>\n",
       "      <td>-0.005218</td>\n",
       "      <td>0.017329</td>\n",
       "      <td>0.025586</td>\n",
       "      <td>0.038852</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>10026</td>\n",
       "      <td>3637443.50</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>-0.002902</td>\n",
       "      <td>-0.030323</td>\n",
       "      <td>-0.002902</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date StockID   MarketCap    Ret_5d   Ret_20d   Ret_60d  Ret_month  \\\n",
       "0 2019-01-31   10026  2899156.25  0.013346  0.008357  0.009807   0.006025   \n",
       "1 2019-02-28   10026  2916624.25 -0.021123  0.029734  0.050473   0.026309   \n",
       "2 2019-03-29   10026  2988574.50 -0.024616 -0.021972  0.025677  -0.010451   \n",
       "3 2019-04-30   10026  2957655.75  0.020867  0.018959  0.031149   0.023349   \n",
       "4 2019-05-31   10026  3026714.75  0.020018  0.003693  0.192524   0.003693   \n",
       "5 2019-06-28   10026  3030688.50  0.014726  0.013980  0.193034   0.154645   \n",
       "6 2019-07-31   10026  3501411.50 -0.005218  0.017329  0.025586   0.038852   \n",
       "7 2019-08-30   10026  3637443.50  0.001864 -0.002902 -0.030323  -0.002902   \n",
       "\n",
       "   EWMA_vol  \n",
       "0  0.000988  \n",
       "1  0.000424  \n",
       "2  0.000209  \n",
       "3  0.000136  \n",
       "4  0.000105  \n",
       "5  0.000118  \n",
       "6  0.000963  \n",
       "7  0.000440  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_feather('monthly_20d/20d_month_has_vb_[20]_ma_2018_labels_w_delay.feather').head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
